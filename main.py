import os
import json
import time
import requests
import feedparser
from datetime import datetime
import re
import urllib.parse
from bs4 import BeautifulSoup
from dotenv import load_dotenv

load_dotenv()

GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
DISCORD_WEBHOOK_URL = os.environ.get('DISCORD_WEBHOOK_URL')

# 1. í•´ì™¸ ë‰´ìŠ¤ (êµ¬ê¸€ ë‰´ìŠ¤ RSS - ì˜ì–´/Global)
RSS_QUERIES_GLOBAL = [
    "Artificial Intelligence business trends",
    "Generative AI computer vision startup",
    "Football analytics technology",
    "Sports revenue model innovation",
    "Physical AI robotics market"
]

# 2. êµ­ë‚´ ì •ì±… ë‰´ìŠ¤ (êµ¬ê¸€ ë‰´ìŠ¤ RSS - í•œêµ­ì–´/Domestic Policy)
# ë„¤ì´ë²„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œëŠ” 'ì •ì±…'ë§Œ ì½• ì§‘ì–´ë‚´ê¸° ì–´ë ¤ì›Œì„œ í‚¤ì›Œë“œ ê¸°ë°˜ RSS ì‚¬ìš©
RSS_QUERIES_KR_POLICY = [
    "ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€ AI ì‚°ì—… ìœ¡ì„±",
    "ë²”ì •ë¶€ AI êµ­ê°€ì „ëµ",
    "ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€ ìŠ¤í¬ì¸ ì‚°ì—… ì§€ì›",
    "ìŠ¤í¬ì¸  í…Œí¬ íˆ¬ì í€ë“œ ì •ì±…",
    "ë°ì´í„° ì‚°ì—… ì§„í¥ ë¡œë“œë§µ",
    "AI ìŠ¤íƒ€íŠ¸ì—… ê·œì œ ìƒŒë“œë°•ìŠ¤"
]

# 3. êµ­ë‚´ ì¼ë°˜ ë‰´ìŠ¤ (ë„¤ì´ë²„ ë‰´ìŠ¤ ì„¹ì…˜ ìŠ¤í¬ë˜í•‘)
NAVER_SECTIONS = [
    {"id": "105", "name": "IT/ê³¼í•™"},
    {"id": "101", "name": "ê²½ì œ"}
]

def get_google_news_rss_url(query, region='US'):
    """êµ¬ê¸€ ë‰´ìŠ¤ RSS URL ìƒì„± (ì§€ì—­/ì–¸ì–´ ì„¤ì • ê°€ëŠ¥)"""
    encoded_query = urllib.parse.quote(query)
    
    if region == 'KR':
        # í•œêµ­ì–´/í•œêµ­
        base_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
    else:
        # ì˜ì–´/ë¯¸êµ­ (ê¸°ë³¸)
        base_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
        
    return base_url

def fetch_google_rss_items(queries, region='US', source_label="Google News"):
    """êµ¬ê¸€ RSS ê¸°ë°˜ ë‰´ìŠ¤ ìˆ˜ì§‘ (ê³µí†µ í•¨ìˆ˜)"""
    items = []
    seen_links = set()
    print(f"ğŸ“¡ [{source_label}] RSS ìˆ˜ì§‘ ì¤‘... (Region: {region})")
    
    for query in queries:
        url = get_google_news_rss_url(query, region)
        try:
            feed = feedparser.parse(url)
            # í‚¤ì›Œë“œë³„ ìƒìœ„ 3~5ê°œë§Œ ê°€ì ¸ì™€ì„œ í›„ë³´êµ° êµ¬ì„±
            limit = 5 if region == 'US' else 3 
            for entry in feed.entries[:limit]:
                if entry.link not in seen_links:
                    items.append({
                        "title": entry.title,
                        "link": entry.link,
                        "source": source_label,
                        "snippet": entry.get("description", "")[:200]
                    })
                    seen_links.add(entry.link)
        except Exception as e:
            print(f"Error fetching RSS for {query}: {e}")
            
    return items

def fetch_naver_news():
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ì„¹ì…˜ í¬ë¡¤ë§ (IT/ê³¼í•™, ê²½ì œ)"""
    items = []
    seen_links = set()
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    print("ğŸ“¡ [êµ­ë‚´] ë„¤ì´ë²„ ë‰´ìŠ¤(IT/ê²½ì œ) ìˆ˜ì§‘ ì¤‘...")
    
    for section in NAVER_SECTIONS:
        url = f"https://news.naver.com/section/{section['id']}"
        try:
            response = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            links = soup.find_all("a", href=True)
            
            count = 0
            for link in links:
                href = link['href']
                title = link.get_text(strip=True)
                
                if "/mnews/article/" in href and title and len(title) > 10:
                    if href not in seen_links:
                        items.append({
                            "title": title,
                            "link": href,
                            "source": f"Naver News ({section['name']})",
                            "snippet": "" 
                        })
                        seen_links.add(href)
                        count += 1
                        if count >= 8: # ì„¹ì…˜ë‹¹ ê°œìˆ˜ ì¡°ì ˆ
                            break
        except Exception as e:
            print(f"Naver Fetch Error ({section['name']}): {e}")
            
    return items

def analyze_and_filter_news(news_items):
    if not news_items:
        return []

    print(f"ğŸ§  ì´ {len(news_items)}ê°œì˜ í›„ë³´ ê¸°ì‚¬ ë¶„ì„ ë° ì„ ë³„ ì¤‘...")
    
    model_name = "gemini-1.5-flash"
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={GEMINI_API_KEY}"
    headers = {'Content-Type': 'application/json'}
    
    # í† í° ì ˆì•½ì„ ìœ„í•œ ê°„ì†Œí™”
    simplified_items = []
    for item in news_items:
        simplified_items.append({
            "t": item['title'],
            "l": item['link'],
            "s": item['source']
        })
        
    news_text = json.dumps(simplified_items, ensure_ascii=False)
    
    prompt = f"""
    ë„ˆëŠ” 'ë¹„ì£¼ì–¼ AI(Computer Vision) & ìŠ¤í¬ì¸  í…Œí¬' ìŠ¤íƒ€íŠ¸ì—…ì˜ CEOì•¼.
    ì œê³µëœ ë‰´ìŠ¤ í›„ë³´êµ°(JSON)ì—ì„œ ìš°ë¦¬ ì‚¬ì—…ê³¼ ê´€ë ¨ëœ **í•µì‹¬ ë‰´ìŠ¤**ì™€ **ì¤‘ìš” ì •ë¶€ ì •ì±…**ì„ íë ˆì´ì…˜í•´ì¤˜.

    [í›„ë³´êµ° ë°ì´í„°]:
    {news_text}

    [ì„ ë³„ ê¸°ì¤€ (ìš°ì„ ìˆœìœ„ ìˆœ)]:
    1. **ì •ë¶€ ì •ì±…**: ëŒ€í•œë¯¼êµ­ ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€(AI)ë‚˜ ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€(ìŠ¤í¬ì¸ )ì˜ ì§€ì› ì‚¬ì—…, ê·œì œ ë³€í™”, í€ë“œ ì¡°ì„± ë“± ìŠ¤íƒ€íŠ¸ì—…ì— ì§ì ‘ì  ì˜í–¥ì„ ì£¼ëŠ” ì •ì±… ë‰´ìŠ¤ (ë°œê²¬ ì¦‰ì‹œ í¬í•¨).
    2. **ê¸°ìˆ  íŠ¸ë Œë“œ**: AI, í”¼ì§€ì»¬ ì»´í“¨íŒ…, ë¡œë´‡, ë¹„ì „ ê¸°ìˆ ì˜ ìƒˆë¡œìš´ ëŒíŒŒêµ¬ë‚˜ ì ìš© ì‚¬ë¡€.
    3. **ì‹œì¥ ë™í–¥**: ìŠ¤í¬ì¸  ì‚°ì—…ì˜ ë””ì§€í„¸ ì „í™˜, íˆ¬ì ì†Œì‹.
    4. **ì œì™¸ ëŒ€ìƒ**: ë‹¨ìˆœ ê²½ê¸° ìŠ¤ì½”ì–´, ì—°ì˜ˆì¸ ê°€ì‹­, ì •ì¹˜ ì‹¸ì›€, ë„ˆë¬´ ì¼ë°˜ì ì¸ ì£¼ê°€ ë³€ë™.

    [ì‘ì„± ì§€ì¹¨]:
    - **ìˆ˜ëŸ‰**: 7~10ê°œ ë‚´ì™¸. (ì •ì±… ë‰´ìŠ¤ëŠ” ê°€ê¸‰ì  í¬í•¨)
    - **ì–¸ì–´**: í•´ì™¸ ë‰´ìŠ¤ëŠ” ë°˜ë“œì‹œ **í•œêµ­ì–´ë¡œ ë²ˆì—­**í•´ì„œ ìš”ì•½.
    - **ìš”ì•½ë¬¸**: "ì •ë¶€ì˜ AI ì˜ˆì‚°ì´ ì¦ì•¡ë˜ì–´ ìš°ë¦¬ R&D ê³¼ì œ ì§€ì›ì´ ìœ ë¦¬í•´ì§ˆ ì „ë§ì…ë‹ˆë‹¤" ì²˜ëŸ¼ ìŠ¤íƒ€íŠ¸ì—… ì…ì¥ì—ì„œ ì„œìˆ .

    [ì¶œë ¥ í¬ë§· - JSON Array Only]:
    [
      {{
        "title": "ê¸°ì‚¬ ì œëª©",
        "summary": "í•µì‹¬ ì¸ì‚¬ì´íŠ¸ (í•œêµ­ì–´)",
        "original_link": "ë§í¬",
        "source": "ì¶œì²˜ í‘œê¸° (ì˜ˆ: [ì •ì±…], [í•´ì™¸], [ë„¤ì´ë²„])"
      }}
    ]
    """
    
    data = {"contents": [{"parts": [{"text": prompt}]}]}
    
    try:
        response = requests.post(url, headers=headers, json=data)
        if response.status_code == 200:
            text = response.json().get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '[]')
            clean_text = re.sub(r"```json|```", "", text).strip()
            # JSON íŒŒì‹± ì‹œë„ (ëŒ€ê´„í˜¸ ì°¾ê¸°)
            match = re.search(r'\[.*\]', clean_text, re.DOTALL)
            if match:
                return json.loads(match.group(0))
            return json.loads(clean_text)
    except Exception as e:
        print(f"Gemini API Error: {e}")
        return []
    
    return []

def send_discord_report(news_list):
    if not DISCORD_WEBHOOK_URL:
        print("ë””ìŠ¤ì½”ë“œ ì›¹í›… URL ì—†ìŒ")
        return
    if not news_list:
        print("ì „ì†¡í•  ë‰´ìŠ¤ê°€ ì—†ìŒ")
        return

    today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
    
    embed = {
        "title": f"ğŸ“° {today} AIÂ·ìŠ¤í¬ì¸  ìŠ¤íƒ€íŠ¸ì—… ë°ì¼ë¦¬ ë¸Œë¦¬í•‘",
        "description": "êµ­ë‚´ì™¸ ì‚°ì—… ë™í–¥ ë° ì£¼ìš” ì •ë¶€ ì •ì±… ëª¨ë‹ˆí„°ë§",
        "color": 0x00ff00,
        "fields": [],
        "footer": {
            "text": "Powered by Gemini Agent",
        }
    }
    
    for news in news_list:
        # ì†ŒìŠ¤ì— ë”°ë¼ ì•„ì´ì½˜ì´ë‚˜ íƒœê·¸ë¥¼ ë‹¤ë¥´ê²Œ í•  ìˆ˜ë„ ìˆìŒ
        source_display = news.get('source', 'ë‰´ìŠ¤')
        # AIê°€ sourceë¥¼ ë®ì–´ì“°ì§€ ì•Šì•˜ë‹¤ë©´ ì›ë³¸ source ì‚¬ìš©
        if '[' not in source_display and 'Google' in source_display and 'KR' in source_display:
             source_display = "[ì •ì±…]"
        elif '[' not in source_display and 'Google' in source_display:
             source_display = "[í•´ì™¸]"
        elif '[' not in source_display and 'Naver' in source_display:
             source_display = "[êµ­ë‚´]"
             
        embed["fields"].append({
            "name": f"{source_display} {news['title']}",
            "value": f"{news['summary']}\n[ğŸ”— ê¸°ì‚¬ ì½ê¸°]({news['original_link']})",
            "inline": False
        })
        
    payload = {"embeds": [embed]}
    
    try:
        requests.post(DISCORD_WEBHOOK_URL, json=payload)
        print("âœ… ë””ìŠ¤ì½”ë“œ ì „ì†¡ ì™„ë£Œ")
    except Exception as e:
        print(f"ë””ìŠ¤ì½”ë“œ ì „ì†¡ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    # 1. ìˆ˜ì§‘
    # 1-1. í•´ì™¸ (ì˜ì–´)
    overseas_items = fetch_google_rss_items(RSS_QUERIES_GLOBAL, region='US', source_label="[í•´ì™¸]")
    
    # 1-2. êµ­ë‚´ ì •ì±… (í•œêµ­ì–´ í‚¤ì›Œë“œ ê²€ìƒ‰)
    policy_items = fetch_google_rss_items(RSS_QUERIES_KR_POLICY, region='KR', source_label="[ì •ì±…]")
    
    # 1-3. êµ­ë‚´ ì¼ë°˜ (ë„¤ì´ë²„ ì„¹ì…˜)
    domestic_items = fetch_naver_news()
    
    all_items = overseas_items + policy_items + domestic_items
    
    if all_items:
        # 2. ë¶„ì„
        selected = analyze_and_filter_news(all_items)
        if selected:
            print(f"ğŸ‘‰ ìµœì¢… ì„ ë³„ëœ ë‰´ìŠ¤: {len(selected)}ê±´")
            send_discord_report(selected)
        else:
            print("ğŸ¤” ì¡°ê±´ì— ë§ëŠ” ë‰´ìŠ¤ê°€ ì—†ì–´ ì „ì†¡í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
